{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Optimization Attempts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attempt 1: further consolidation of the features, addition of another hidden layer, bump up the node count\n",
    "###  Preprocessing the Data for a Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our dependencies\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler,OneHotEncoder\n",
    "# Import checkpoint dependencies\n",
    "import os\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import preproc_funcs as pf\n",
    "\n",
    "#  Import and read the charity_data.csv.\n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Split our preprocessed data into our features and target arrays\n",
    "y = model_data_df.IS_SUCCESSFUL.values\n",
    "X = model_data_df.drop([\"IS_SUCCESSFUL\"],1).values\n",
    "\n",
    "# Split the preprocessed data into a training and testing dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a StandardScaler instances\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the StandardScaler\n",
    "X_scaler = scaler.fit(X_train)\n",
    "\n",
    "# Scale the data\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check the balance of the training target\n",
    "from collections import Counter\n",
    "Counter(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile, Train and Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model - deep neural net, i.e., the number of input features and hidden nodes for each layer.\n",
    "number_input_features = len(X_train[0])\n",
    "hidden_nodes_layer1 = 66\n",
    "hidden_nodes_layer2 = 33\n",
    "hidden_nodes_layer3 = 10\n",
    "\n",
    "nn_new = tf.keras.models.Sequential()\n",
    "\n",
    "# First hidden layer\n",
    "nn_new.add(\n",
    "    tf.keras.layers.Dense(\n",
    "        units=hidden_nodes_layer1, \n",
    "        input_dim=number_input_features, \n",
    "        activation=\"relu\"\n",
    "    )\n",
    ")\n",
    "# Second hidden layer\n",
    "nn_new.add(tf.keras.layers.Dense(\n",
    "    units=hidden_nodes_layer2, \n",
    "    activation=\"relu\"\n",
    "))\n",
    "\n",
    "# Third hidden layer\n",
    "nn_new.add(tf.keras.layers.Dense(\n",
    "    units=hidden_nodes_layer3, \n",
    "    activation=\"relu\"\n",
    "))\n",
    "\n",
    "# Output layer\n",
    "nn_new.add(tf.keras.layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "\n",
    "# Check the structure of the model\n",
    "nn_new.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "nn_new.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the checkpoint path and filenames\n",
    "os.makedirs(\"checkpoints_new/\",exist_ok=True)\n",
    "checkpoint_path = \"checkpoints_new/weights.{epoch:02d}.hdf5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a callback that saves the model's weights every epoch\n",
    "cp_callback = ModelCheckpoint(\n",
    "    filepath=checkpoint_path,\n",
    "    verbose=1,\n",
    "    save_weights_only=True,\n",
    "    period=5)\n",
    "\n",
    "# Train the model\n",
    "fit_model = nn_new.fit(X_train_scaled,y_train,epochs=50,callbacks=[cp_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model using the test data\n",
    "model_loss, model_accuracy = nn_new.evaluate(X_test_scaled,y_test,verbose=2)\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the model to HDF5 file\n",
    "nn_new.save(\"trained_application_new_v1.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame containing training history\n",
    "history_df = pd.DataFrame(fit_model.history, index=range(1,len(fit_model.history[\"loss\"])+1))\n",
    "\n",
    "# Plot the loss\n",
    "history_df.plot(y=\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot the accuracy\n",
    "history_df.plot(y=\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attempt 2: Give the model all of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Split our preprocessed data into our features and target arrays\n",
    "y = model_data_df.IS_SUCCESSFUL.values\n",
    "X = model_data_df.drop([\"IS_SUCCESSFUL\"],1).values\n",
    "\n",
    "# Split the preprocessed data into a training and testing dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a StandardScaler instances\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the StandardScaler\n",
    "X_scaler = scaler.fit(X_train)\n",
    "\n",
    "# Scale the data\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile, Train and Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model - deep neural net, i.e., the number of input features and hidden nodes for each layer.\n",
    "number_input_features = len(X_train[0])\n",
    "hidden_nodes_layer1 = 3*number_input_features\n",
    "hidden_nodes_layer2 = 150\n",
    "hidden_nodes_layer3 = 50\n",
    "\n",
    "nn_new_v2 = tf.keras.models.Sequential()\n",
    "\n",
    "# First hidden layer\n",
    "nn_new_v2.add(\n",
    "    tf.keras.layers.Dense(\n",
    "        units=hidden_nodes_layer1, \n",
    "        input_dim=number_input_features, \n",
    "        activation=\"relu\"\n",
    "    )\n",
    ")\n",
    "# Second hidden layer\n",
    "nn_new_v2.add(tf.keras.layers.Dense(\n",
    "    units=hidden_nodes_layer2, \n",
    "    activation=\"relu\"\n",
    "))\n",
    "\n",
    "# Third hidden layer\n",
    "nn_new_v2.add(tf.keras.layers.Dense(\n",
    "    units=hidden_nodes_layer3, \n",
    "    activation=\"relu\"\n",
    "))\n",
    "\n",
    "# Output layer\n",
    "nn_new_v2.add(tf.keras.layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "\n",
    "# Check the structure of the model\n",
    "nn_new_v2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "nn_new_v2.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the checkpoint path and filenames\n",
    "os.makedirs(\"checkpoints_new_v2/\",exist_ok=True)\n",
    "checkpoint_path = \"checkpoints_new_v2/weights.{epoch:02d}.hdf5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a callback that saves the model's weights every epoch\n",
    "cp_callback = ModelCheckpoint(\n",
    "    filepath=checkpoint_path,\n",
    "    verbose=1,\n",
    "    save_weights_only=True,\n",
    "    period=5)\n",
    "\n",
    "# Train the model\n",
    "fit_model = nn_new_v2.fit(X_train_scaled,y_train,epochs=200,callbacks=[cp_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model using the test data\n",
    "model_loss, model_accuracy = nn_new_v2.evaluate(X_test_scaled,y_test,verbose=2)\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the model to HDF5 file\n",
    "nn_new_v2.save(\"trained_application_new_v2.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame containing training history\n",
    "history_df = pd.DataFrame(fit_model.history, index=range(1,len(fit_model.history[\"loss\"])+1))\n",
    "\n",
    "# Plot the loss\n",
    "history_df.plot(y=\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot the accuracy\n",
    "history_df.plot(y=\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attempt 3: give the model all the data but tweak model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Split our preprocessed data into our features and target arrays\n",
    "y = model_data_df.IS_SUCCESSFUL.values\n",
    "X = model_data_df.drop([\"IS_SUCCESSFUL\"],1).values\n",
    "\n",
    "# Split the preprocessed data into a training and testing dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a StandardScaler instances\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the StandardScaler\n",
    "X_scaler = scaler.fit(X_train)\n",
    "\n",
    "# Scale the data\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile, Train and Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model - deep neural net, i.e., the number of input features and hidden nodes for each layer.\n",
    "number_input_features = len(X_train[0])\n",
    "hidden_nodes_layer1 = 50\n",
    "hidden_nodes_layer2 = 10\n",
    "\n",
    "nn_new_v3 = tf.keras.models.Sequential()\n",
    "\n",
    "# First hidden layer\n",
    "nn_new_v3.add(\n",
    "    tf.keras.layers.Dense(\n",
    "        units=hidden_nodes_layer1, \n",
    "        input_dim=number_input_features, \n",
    "        activation=\"relu\"\n",
    "    )\n",
    ")\n",
    "# Second hidden layer\n",
    "nn_new_v3.add(tf.keras.layers.Dense(\n",
    "    units=hidden_nodes_layer2, \n",
    "    activation=\"relu\"\n",
    "))\n",
    "\n",
    "# Output layer\n",
    "nn_new_v3.add(tf.keras.layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "\n",
    "# Check the structure of the model\n",
    "nn_new_v3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "nn_new_v3.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the checkpoint path and filenames\n",
    "os.makedirs(\"checkpoints_new_v3/\",exist_ok=True)\n",
    "checkpoint_path = \"checkpoints_new_v3/weights.{epoch:02d}.hdf5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a callback that saves the model's weights every epoch\n",
    "cp_callback = ModelCheckpoint(\n",
    "    filepath=checkpoint_path,\n",
    "    verbose=1,\n",
    "    save_weights_only=True,\n",
    "    period=5)\n",
    "\n",
    "# Train the model\n",
    "fit_model = nn_new_v3.fit(X_train_scaled,y_train,epochs=50,callbacks=[cp_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model using the test data\n",
    "model_loss, model_accuracy = nn_new_v3.evaluate(X_test_scaled,y_test,verbose=2)\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the model to HDF5 file\n",
    "nn_new_v3.save(\"trained_application_new_v3.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame containing training history\n",
    "history_df = pd.DataFrame(fit_model.history, index=range(1,len(fit_model.history[\"loss\"])+1))\n",
    "\n",
    "# Plot the loss\n",
    "history_df.plot(y=\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot the accuracy\n",
    "history_df.plot(y=\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Atempt 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model - deep neural net, i.e., the number of input features and hidden nodes for each layer.\n",
    "number_input_features = len(X_train[0])\n",
    "hidden_nodes_layer1 = 150\n",
    "hidden_nodes_layer2 = 75\n",
    "hidden_nodes_layer3 = 50\n",
    "hidden_nodes_layer4 = 25\n",
    "\n",
    "nn_new_v4 = tf.keras.models.Sequential()\n",
    "\n",
    "# First hidden layer\n",
    "nn_new_v4.add(\n",
    "    tf.keras.layers.Dense(\n",
    "        units=hidden_nodes_layer1, \n",
    "        input_dim=number_input_features, \n",
    "        activation=\"relu\"\n",
    "    )\n",
    ")\n",
    "# Second hidden layer\n",
    "nn_new_v4.add(tf.keras.layers.Dense(\n",
    "    units=hidden_nodes_layer2, \n",
    "    activation=\"relu\"\n",
    "))\n",
    "\n",
    "# Second hidden layer\n",
    "nn_new_v4.add(tf.keras.layers.Dense(\n",
    "    units=hidden_nodes_layer2, \n",
    "    activation=\"relu\"\n",
    "))\n",
    "\n",
    "# Second hidden layer\n",
    "nn_new_v4.add(tf.keras.layers.Dense(\n",
    "    units=hidden_nodes_layer2, \n",
    "    activation=\"relu\"\n",
    "))\n",
    "\n",
    "# Output layer\n",
    "nn_new_v4.add(tf.keras.layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "\n",
    "# Check the structure of the model\n",
    "nn_new_v4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "nn_new_v4.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import checkpoint dependencies\n",
    "import os\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# Define the checkpoint path and filenames\n",
    "os.makedirs(\"checkpoints_new_v4/\",exist_ok=True)\n",
    "checkpoint_path = \"checkpoints_new_v4/weights.{epoch:02d}.hdf5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a callback that saves the model's weights every epoch\n",
    "cp_callback = ModelCheckpoint(\n",
    "    filepath=checkpoint_path,\n",
    "    verbose=1,\n",
    "    save_weights_only=True,\n",
    "    period=5)\n",
    "\n",
    "# Train the model\n",
    "fit_model = nn_new_v4.fit(X_train_scaled,y_train,epochs=50,callbacks=[cp_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model using the test data\n",
    "model_loss, model_accuracy = nn_new_v4.evaluate(X_test_scaled,y_test,verbose=2)\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the model to HDF5 file\n",
    "nn_new_v4.save(\"trained_application_new_v4.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame containing training history\n",
    "history_df = pd.DataFrame(fit_model.history, index=range(1,len(fit_model.history[\"loss\"])+1))\n",
    "\n",
    "# Plot the loss\n",
    "history_df.plot(y=\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot the accuracy\n",
    "history_df.plot(y=\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attempt 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Split our preprocessed data into our features and target arrays\n",
    "y = model_data_df.IS_SUCCESSFUL.values\n",
    "X = model_data_df.drop([\"IS_SUCCESSFUL\"],1).values\n",
    "\n",
    "# Split the preprocessed data into a training and testing dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model - deep neural net, i.e., the number of input features and hidden nodes for each layer.\n",
    "number_input_features = len(X_train[0])\n",
    "hidden_nodes_layer1 = 3*number_input_features\n",
    "hidden_nodes_layer2 = np.floor(0.5*hidden_nodes_layer1)\n",
    "hidden_nodes_layer3 = np.floor(0.5*hidden_nodes_layer1)\n",
    "hidden_nodes_layer4 = np.floor(0.5*hidden_nodes_layer2)\n",
    "\n",
    "nn_new_v5 = tf.keras.models.Sequential()\n",
    "\n",
    "# First hidden layer\n",
    "nn_new_v5.add(\n",
    "    tf.keras.layers.Dense(\n",
    "        units=hidden_nodes_layer1, \n",
    "        input_dim=number_input_features, \n",
    "        activation=\"relu\"\n",
    "    )\n",
    ")\n",
    "# Second hidden layer\n",
    "nn_new_v5.add(tf.keras.layers.Dense(\n",
    "    units=hidden_nodes_layer2, \n",
    "    activation=\"relu\"\n",
    "))\n",
    "\n",
    "# Second hidden layer\n",
    "nn_new_v5.add(tf.keras.layers.Dense(\n",
    "    units=hidden_nodes_layer2, \n",
    "    activation=\"relu\"\n",
    "))\n",
    "\n",
    "# Second hidden layer\n",
    "nn_new_v5.add(tf.keras.layers.Dense(\n",
    "    units=hidden_nodes_layer2, \n",
    "    activation=\"relu\"\n",
    "))\n",
    "\n",
    "# Output layer\n",
    "nn_new_v5.add(tf.keras.layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "\n",
    "# Check the structure of the model\n",
    "nn_new_v5.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "nn_new_v5.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the checkpoint path and filenames\n",
    "os.makedirs(\"checkpoints_new_v5/\",exist_ok=True)\n",
    "checkpoint_path = \"checkpoints_new_v5/weights.{epoch:02d}.hdf5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a callback that saves the model's weights every epoch\n",
    "cp_callback = ModelCheckpoint(\n",
    "    filepath=checkpoint_path,\n",
    "    verbose=1,\n",
    "    save_weights_only=True,\n",
    "    period=5)\n",
    "\n",
    "# Train the model\n",
    "fit_model = nn_new_v5.fit(X_train_scaled,y_train,epochs=50,callbacks=[cp_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model using the test data\n",
    "model_loss, model_accuracy = nn_new_v5.evaluate(X_test_scaled,y_test,verbose=2)\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the model to HDF5 file\n",
    "nn_new_v5.save(\"trained_application_new_v5.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame containing training history\n",
    "history_df = pd.DataFrame(fit_model.history, index=range(1,len(fit_model.history[\"loss\"])+1))\n",
    "\n",
    "# Plot the loss\n",
    "history_df.plot(y=\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot the accuracy\n",
    "history_df.plot(y=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "mlenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
